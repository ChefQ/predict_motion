{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note \n",
    "You will have to run this notebook six times:\n",
    "1. Support with Tfidf\n",
    "2. Support with Embeddings\n",
    "3. Opposition with Tfidf\n",
    "4. Opposition with Embeddings\n",
    "5. Both with Tfidf\n",
    "6. Both with Embeddings\n",
    "\n",
    "\n",
    "The variables to pay attention too are `feature` and `key`\n",
    "\n",
    "`key` can be support , oppostion or both\n",
    "`feature` can be tfidf or embedding\n",
    "\n",
    "\n",
    "if  `key` is both then set  `both` in  `Data(testset[testset['data_type'] == 'train'], feature=feature ,both=?)` to `True`\n",
    "\n",
    "e.g Data(testset[testset['data_type'] == 'train'], feature=feature ,both=True)\n",
    "\n",
    "Else if `key` is not both, thus `support` or `opposition` then set `both` to `False`\n",
    "\n",
    "Data(testset[testset['data_type'] == 'train'], feature=feature ,both=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "from joblib import dump, load\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Set, Dict, Tuple\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import (\n",
    "    Sequential as Seq,\n",
    "    Linear as Lin,\n",
    "    ReLU,\n",
    "    BatchNorm1d,\n",
    "    AvgPool1d,\n",
    "    Sigmoid,\n",
    "    Conv1d,\n",
    ")\n",
    "import wandb\n",
    "\n",
    "from deepsetmodel import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Here i load a paired datatset.\n",
    "\n",
    "Unlike the other (unpaird) datatset which contains individual briefs. \n",
    "\n",
    "Each row of the paired datast contains two set of briefs. Where each set corresponds to either oppostion and support.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51, 8), (199, 8))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICENUMBER = 4\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(F\"cuda:{DEVICENUMBER}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "llm_checkpoint = \"allenai/longformer-large-4096\"\n",
    "\n",
    "PAIRED_PATH = '../dataset/paired_testset.csv' #'../summaries/summarized_paired_testset.csv' # '../dataset/paired_testset.csv'\n",
    "UNPAIR_PATH = '../dataset/testset.csv' \n",
    "\n",
    "testset = pd.read_csv(PAIRED_PATH, sep=',',index_col=0)\n",
    "unpaired = pd.read_csv(UNPAIR_PATH, sep=',',index_col=0)\n",
    "\n",
    "testset = testset.loc[testset['data_type'] == 'train']\n",
    "\n",
    "# randomly set 20% of the data to test\n",
    "testset['data_type'] = testset['data_type'].apply(lambda x: \"test\" if random.random() < 0.2 else \"train\")\n",
    "testset[testset['data_type'] == 'test'].shape , testset[testset['data_type'] == 'train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader *\n",
    "\n",
    "Just like the LLM Notebook you will have to run the cells below multiple times to run different experiments.\n",
    "\n",
    "As of the time this note was written,  If you want to specify the input type (\"tfidf\" or \"embedding\"), then set feature variable below.\n",
    "\n",
    "The Data class returns a pytorch dataset object.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0da4bcfedb47a3974c69229e37fb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Features, Value, ClassLabel ,  Dataset\n",
    "\n",
    "\n",
    "\n",
    "features = Features({ 'prompt' : Value(dtype='string'),\n",
    "                     'completion': ClassLabel(num_classes=3, names=['deny', 'grant', 'TBD'],  id=None),\n",
    "                     'brief_type' : ClassLabel(num_classes=2, names=[\"support\", \"opposition\"], id=None),\n",
    "                        'data_type' : ClassLabel(num_classes=2, names=[\"train\", \"test\"], id=None),\n",
    "                        'file_path' : Value(dtype='int64') ,\n",
    "                        'file_name' : Value(dtype='string'),   \n",
    "                        'labels' : Value(dtype='int64')\n",
    "                     })\n",
    "\n",
    "# can change the argument\n",
    "   \n",
    "dataset = Dataset.from_pandas(unpaired, preserve_index=False  )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_checkpoint)\n",
    "\n",
    "def tokenize_function(briefs):\n",
    "    return tokenizer(briefs[\"prompt\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(200))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"completion\",\"prompt\",\"brief_type\",\"data_type\", \"file_path\",\"file_name\" ]) # \"file_name\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=2, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch_example to the first batch of the training dataloader\n",
    "\n",
    "batch_example = next(iter(train_dataloader))\n",
    "batch_example['input_ids'].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "# load model that outputs embeddings\n",
    "model = AutoModel.from_pretrained(llm_checkpoint)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in batch_example.items() if k != \"file_name\"}\n",
    "\n",
    "output = model(**inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4096, 1024])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data class definition\n",
    "\n",
    "Below is the signature of the Data class:\n",
    "\n",
    "<font color=\"blue\">def __init__(self,df,feature = 'tfidf', getEmbeddings = sentence_model.encode ,support_pipe = '../pipes/support-tfidf.joblib', opposition_pipe = '../pipes/oppose-tfidf.joblib', both =False, both_pipe = '../pipes/both-tfidf.joblib')</font>\n",
    "\n",
    "`self.feature`: This specifies the feature engineering technique to use on the dataset. \n",
    "                There are only 2 feature types implemented: [\"tfidf\", \"embeddings\"]\n",
    "\n",
    "\n",
    "`self.getEmbeddings`: This refers to the embedding function that will be used on the text. You can pick use your own embedding function. You don't have to specify an embedding function the default is already specified for you. \n",
    "\n",
    "`self.both`: When set to True a dataset where the supporting and opposing briefs are joined (Unioned) together to from one set of briefs.\n",
    "             When Set to False a dataset of disjointed supporting and oppossing breifs are joined. \n",
    "             You have to make sure that if you set this to true, then you have to set the configuration for the model to accept datasets that are joined.\n",
    "\n",
    "`self.support_pipe, self.opposition_pipe`: These are paths to pipes that convert text to tfidf vectors. Default paths are provided, so you don't have to explicitly set it.\n",
    "\n",
    "`self.both_pipe`: This is a path that converst text to tfidf vectors. This requires particular attention, because this pipe is constructed from both support and opposition. It is does a requirement is you set `self.both` to True. Default paths are provided, so you don't have to explicitly set it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples and Usecase:\n",
    "\n",
    "Example 1.\n",
    "<font color=\"blue\">train_data = Data(testset[testset['data_type'] == 'train'], feature='embedding' ,both=True)</font>:\n",
    "\n",
    "returns an embedding dataset where support and oppositions are combined.\n",
    "\n",
    "Example 2.\n",
    "<font color=\"blue\"> train_data = Data(testset[testset['data_type'] == 'test'], feature='tfidf' ,both=False) </font>:\n",
    "\n",
    "returns a tfidf dataset where where support and opposiontions are seperated.\n",
    "\n",
    "Example 3.\n",
    "```\n",
    "def embedding_func(brief):\n",
    "    \n",
    "    ....\n",
    "```\n",
    "<font color=\"blue\"> train_data = Data(testset[testset['data_type'] == 'test'], feature='embedding',both=False , getEmbeddings = embedding_func ): </font>\n",
    "\n",
    "returns a embedding dataset where the embedding function is explicitly defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self,df,feature = 'tfidf', getEmbeddings = sentence_model.encode ,support_pipe = '../pipes/support-tfidf.joblib', opposition_pipe = '../pipes/oppose-tfidf.joblib', both =False, both_pipe = '../pipes/both-tfidf.joblib'):\n",
    "        self.df = df    \n",
    "        supports = self.df['support'].values\n",
    "        oppositions = self.df['opposition'].values\n",
    "        self.folder_id = self.df['folder_id'].values\n",
    "        self.y = self.df['outcome'].values \n",
    "        # convert list of stings to list of lists of stings\n",
    "        supports = list(map(lambda x: ast.literal_eval(x), supports))\n",
    "        oppositions = list(map(lambda x: ast.literal_eval(x), oppositions))\n",
    "        self.both = both\n",
    "        if self.both:\n",
    "            self.combined = list(map(lambda x,y: x+y, supports,oppositions))\n",
    "\n",
    "\n",
    "        self.getEmbeddings = getEmbeddings\n",
    "        \n",
    "        if self.both == False:\n",
    "            self.max_len_brief = max(self.findMaxLen(supports),self.findMaxLen(oppositions))\n",
    "        else:\n",
    "            self.max_len_brief = self.findMaxLen(self.combined)\n",
    "\n",
    "        if feature == 'tfidf':\n",
    "            if self.both == False:\n",
    "                support_pipe = load(support_pipe)\n",
    "                opposition_pipe = load(opposition_pipe)\n",
    "                getSupport = lambda x: self.stringsToTfidfs(x,support_pipe)\n",
    "                getOpposition = lambda x: self.stringsToTfidfs(x,opposition_pipe)\n",
    "\n",
    "\n",
    "                self.supports = list(map( getSupport, supports))\n",
    "                self.oppositions = list(map( getOpposition, oppositions))\n",
    "\n",
    "            else:\n",
    "                both_pipe = load(both_pipe)\n",
    "                getTfidf= lambda x: self.stringsToTfidfs(x,both_pipe)\n",
    "                self.combined = list(map( getTfidf, self.combined))\n",
    "\n",
    "        elif feature == 'embedding':\n",
    "            if self.both == False:\n",
    "                self.supports: list = list(map(lambda x: self.stringsToEmbeddings(x), supports))\n",
    "                self.oppositions: list = list(map(lambda x: self.stringsToEmbeddings(x), oppositions))\n",
    "            else:\n",
    "                self.combined: list = list(map(lambda x: self.stringsToEmbeddings(x), self.combined))\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.both == False:\n",
    "            return len(self.supports)\n",
    "        else:\n",
    "            return len(self.combined)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = 1.0 if self.y[idx] == 'grant' else 0.0\n",
    "\n",
    "        if hasattr(self, 'combined') and self.both == True:\n",
    "            return self.combined[idx] , y , self.folder_id[idx]\n",
    "        else:\n",
    "            return self.supports[idx] , self.oppositions[idx] , y , self.folder_id[idx]\n",
    "        \n",
    "    def findMaxLen(self,x):\n",
    "        max_len = 0\n",
    "        for i in range(len(x)):\n",
    "            row = x[i]\n",
    "            if len(row) > max_len:\n",
    "                max_len = len(row)\n",
    "        return max_len\n",
    "\n",
    "    def stringsToTfidfs(self,briefs: List[str],pipe):\n",
    "        tfidfs = torch.tensor(pipe.transform(briefs).toarray(),dtype=torch.float32)\n",
    "\n",
    "        return self.padFeatures(tfidfs)\n",
    "    \n",
    "\n",
    "    \n",
    "    def stringsToEmbeddings(self,briefs: List[str]):\n",
    "        embeddings =  torch.tensor(self.getEmbeddings(briefs),dtype=torch.float32)\n",
    "        return self.padFeatures(embeddings)\n",
    "    \n",
    "    def padFeatures(self,features: List[torch.tensor]):\n",
    "        num_padding = self.max_len_brief - features.shape[0]\n",
    "        padding = nn.ConstantPad2d((0, 0, 0, num_padding), 0)\n",
    "        features = padding(features)\n",
    "        features = features.T\n",
    "        return features\n",
    "    \n",
    "\n",
    "feature = 'embedding' # 'tfidf' or embedding\n",
    "\n",
    "train_data = Data(testset[testset['data_type'] == 'train'], feature=feature ,both=False)\n",
    "test_data = Data(testset[testset['data_type'] == 'test'], feature=feature ,both=False)\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# warning about pickle version *Vectorizer from version 1.3.2 when using version 1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation *\n",
    "\n",
    "In the cell below is where the size of the model is defined.\n",
    "Here you have to ability to increase or decreace the number of hidden units per layer.\n",
    "\n",
    "Just like the Dataloader tab above you have to run this cell once per experiment.\n",
    "\n",
    "You can only select one out of three types of models [\"support\", \"opposition\",\"both\"]\n",
    "\n",
    "specify your choice in the variable key\n",
    "\n",
    "Example\n",
    "``` key = \"both\" ```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Notes to myself:\n",
    "\n",
    "The performance of the DeepSet model maybe hampered because of the small latent space.\n",
    "The machie i am using cannot handle a larger latent space. Once you get a larger GPU memory retry this scipt with a larger latent space\n",
    "\n",
    "\n",
    "\n",
    "Ideas on reducing the load on GPUs:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html\n",
    "\n",
    "When you have time learn this:\n",
    "\n",
    "https://docs.wandb.ai/guides/model_registry/walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TFIDF is quiet big so i may have to reduce the hiden layers width\n",
    "# the latent space has the be atleast the size of the input\n",
    "\n",
    "models = {}\n",
    "optimizers = {}\n",
    "\n",
    "key = \"support\"\n",
    "\n",
    "if key == \"support\":\n",
    "    input_size = train_data.supports[0].shape[0]\n",
    "elif key == \"opposition\":\n",
    "    input_size = train_data.oppositions[0].shape[0]\n",
    "else: #key == \"both\"    \n",
    "    input_size = train_data.combined[0].shape[0]\n",
    "\n",
    "\n",
    "max_len_brief = train_data.max_len_brief\n",
    "\n",
    "hidden1 = int(input_size /5)\n",
    "hidden2 = int(hidden1 / 4)\n",
    "hidden3 = int(hidden2 / 3)\n",
    "classify1 = int(hidden3 /2)\n",
    "\n",
    "models[key] = DeepSets(input_size, max_len_brief , hidden1, hidden2, hidden3, classify1).to(device)\n",
    "\n",
    "latent_size = int(input_size / 10)\n",
    "hidden_size = latent_size\n",
    "output_size =  1\n",
    "\n",
    "\n",
    "## what does Bachnorm and conv1d work?\n",
    "lr = 1e-4   \n",
    "optimizers[key] = torch.optim.Adam(models[key].parameters(), lr=lr)\n",
    "#optimizers[\"opposition\"] = torch.optim.Adam(models[\"opposition\"].parameters(), lr=1e-4)\n",
    "#optimizers[\"both\"] = torch.optim.Adam(models[\"both\"].parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "These are functions for  training, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, total, batch_size, leave=False , datatype='support', loss_fn= nn.BCELoss()):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "\n",
    "    t = tqdm(enumerate(loader), total=total /batch_size, leave=leave)\n",
    "\n",
    "    csv = {'folder':[],'prediction':[], 'score':[], 'truth':[]}\n",
    "\n",
    "    for i, data in t:\n",
    "\n",
    "        if datatype != \"both\":\n",
    "            supports, oppositions, y , folder_id = data\n",
    "            supports = supports.to(device)\n",
    "            oppositions = oppositions.to(device)\n",
    "        else:\n",
    "            combined, y , folder_id = data\n",
    "            combined = combined.to(device)\n",
    "\n",
    "        y = y.float()\n",
    "        y = y.reshape(-1,1)\n",
    "        y = y.to(device)\n",
    "\n",
    "        if datatype == 'support':\n",
    "            outputs= model(supports)\n",
    "        elif datatype == 'opposition':\n",
    "            outputs= model(oppositions)\n",
    "        elif datatype == 'both':\n",
    "            outputs= model(combined)\n",
    "\n",
    "        loss = loss_fn(outputs, y)\n",
    "        predictions = (outputs > 0.5)\n",
    "        acc = (predictions == y).sum().item()\n",
    "        sum_acc += acc\n",
    "        avg_acc =  acc /batch_size\n",
    "        \n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        t.set_description(f\"batch_loss_{datatype}: {loss.item():.4f} \\t| sum_loss_{datatype}: {sum_loss:.4f}\\n batch_accuracy_{datatype}: {avg_acc:.4f}\")\n",
    "        \n",
    "        t.refresh()\n",
    "\n",
    "        csv['folder'].extend(folder_id)\n",
    "        csv['prediction'].extend(predictions.cpu().numpy().flatten())\n",
    "        csv['score'].extend(outputs.cpu().numpy().flatten())\n",
    "        csv['truth'].extend(y.cpu().numpy().flatten())\n",
    "\n",
    "        \n",
    "    # what is the (i+1) for?\n",
    "        \n",
    "    return sum_loss  / len(loader.dataset) , sum_acc / len(loader.dataset) , pd.DataFrame(csv)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader, total, batch_size, leave=False, datatype='support', loss_fn= nn.BCELoss()):\n",
    "    model.train()\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    t = tqdm(enumerate(loader), total=total /batch_size, leave=leave)\n",
    "    for i, data in t:\n",
    "\n",
    "        if key != \"both\":\n",
    "                \n",
    "            supports, oppositions, y , _ = data\n",
    "            supports = supports.to(device)\n",
    "            oppositions = oppositions.to(device)\n",
    "\n",
    "        else:\n",
    "            combined, y , _ = data\n",
    "            combined = combined.to(device)\n",
    "\n",
    "        y = y.float()\n",
    "        y = y.reshape(-1,1)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if datatype == 'support':\n",
    "            outputs= model(supports)\n",
    "        elif datatype == 'opposition':\n",
    "            outputs= model(oppositions)\n",
    "        elif datatype == 'both':\n",
    "            outputs= model(combined)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        #wandb.log({\"batch_loss\": loss.item() } )\n",
    "      \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        t.set_description(f\"batch_loss_{datatype}: {loss.item():.4f} \\t| sum_loss_{datatype}: {sum_loss:.4f}\")\n",
    "        t.refresh()\n",
    "\n",
    "    return sum_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "The cell below is where the model is trained , tested, used for inference and saved to disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5db220b4104302a085510a8aa607b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moqcardoso\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratchB/oqcardoso/predict_motion/per_motion_prediction/wandb/run-20240328_173411-8407lwoz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oqcardoso/DeepSets/runs/8407lwoz' target=\"_blank\">supports-embedding-epochs:300-patience:100 epochs</a></strong> to <a href='https://wandb.ai/oqcardoso/DeepSets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oqcardoso/DeepSets' target=\"_blank\">https://wandb.ai/oqcardoso/DeepSets</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oqcardoso/DeepSets/runs/8407lwoz' target=\"_blank\">https://wandb.ai/oqcardoso/DeepSets/runs/8407lwoz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32bfaa86fca40c69b6a5b0f40962b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cae1e9db9a6475cbacfc78c0a04347f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Training Loss:   0.2372\n",
      "           Validation Loss: 0.3081\n",
      "           Validation Accuracy: 0.3000\n",
      "New best model saved to: ../models/DeepSets_support_embedding.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d4134a5b09445588c553757a671da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacb0c65b5514630a19bd01c709bf5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Training Loss:   0.2324\n",
      "           Validation Loss: 0.3688\n",
      "           Validation Accuracy: 0.3750\n",
      "New best model saved to: ../models/DeepSets_support_embedding.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3aaeb9f195f45ee98dd5a27a960384c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41a28acfe4343bbae47d3daba1c843b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Training Loss:   0.2222\n",
      "           Validation Loss: 0.3255\n",
      "           Validation Accuracy: 0.4500\n",
      "New best model saved to: ../models/DeepSets_support_embedding.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0dfa5b1ca94b53b2b2a87d0652db56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b573efec4f5431a80c500a1d9976b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Training Loss:   0.2256\n",
      "           Validation Loss: 0.3916\n",
      "           Validation Accuracy: 0.3750\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf815c8a1651444e8ec9e45e3d39227f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75307fcb5754c5ab775a344c89e5353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Training Loss:   0.2198\n",
      "           Validation Loss: 0.3474\n",
      "           Validation Accuracy: 0.3500\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf9742c76e743228fd1a925c2b6fd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc323deb73ef474797a268e705101df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Training Loss:   0.2223\n",
      "           Validation Loss: 0.3477\n",
      "           Validation Accuracy: 0.4250\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed7af7c315947e2934347ad2facb4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6991c3a05a49e1b059c31a2e4dad2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Training Loss:   0.2154\n",
      "           Validation Loss: 0.3465\n",
      "           Validation Accuracy: 0.3000\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbabb129e844198bf24f41fd74905b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da96755a8724ca0b44d97b3ff5c156b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Training Loss:   0.2186\n",
      "           Validation Loss: 0.3433\n",
      "           Validation Accuracy: 0.3250\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf187f42a4a46b48d34e362f080a72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f770b65bb0ff418bb958452499e4ede1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Training Loss:   0.2135\n",
      "           Validation Loss: 0.3375\n",
      "           Validation Accuracy: 0.3500\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2090b61683e4ca9986020a7b60f1900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbe168ede384c688ab90f1239d65582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Training Loss:   0.2161\n",
      "           Validation Loss: 0.3197\n",
      "           Validation Accuracy: 0.3250\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5e7c5a1ade42dbaaf9dbeb3532ba28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50db27191b56491bb63b6488cfc30d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss:   0.2043\n",
      "           Validation Loss: 0.3226\n",
      "           Validation Accuracy: 0.3250\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a65a80341d477a8b254e659bd0a4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c82b9e2bf24e508d98fcbd8ffac0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training Loss:   0.2193\n",
      "           Validation Loss: 0.3056\n",
      "           Validation Accuracy: 0.3750\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158a7be1fa474abe9f5fc98ccd002109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230299a47c3f4d2fbf268b35a0f04b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13.333333333333334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training Loss:   0.2140\n",
      "           Validation Loss: 0.3389\n",
      "           Validation Accuracy: 0.3750\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5660c6b7dd4beb8608710957503809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     10\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# set the wandb project where this run will be logged\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSets\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     }\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[0;32m---> 45\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     valid_loss, valid_acc , csv \u001b[38;5;241m=\u001b[39m test(\n\u001b[1;32m     57\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodels[key],\n\u001b[1;32m     58\u001b[0m         loader\u001b[38;5;241m=\u001b[39mtest_loader, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m         datatype\u001b[38;5;241m=\u001b[39mkey\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     65\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: valid_acc})\n",
      "Cell \u001b[0;32mIn[5], line 83\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loader, total, batch_size, leave, datatype, loss_fn)\u001b[0m\n\u001b[1;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datatype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupports\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m datatype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopposition\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     85\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m model(oppositions)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/predict_motion/per_motion_prediction/deepsetmodel.py:85\u001b[0m, in \u001b[0;36mDeepSets.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 85\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# indices = torch.LongTensor(np.zeros(self.narguments)).to(self.device)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m#out = scatter_mean(out, indices, dim=-1)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "n_epochs = 300\n",
    "stale_epochs = 0\n",
    "best_valid_acc = 0.0\n",
    "patience = 100\n",
    "t = tqdm(range(0, n_epochs))\n",
    "  \n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"DeepSets\",  \n",
    "    name= f\"{key}s-{feature}-epochs:{n_epochs}-patience:{patience} epochs\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \n",
    "    \"lr\": lr,\n",
    "\n",
    "    \"dataset\": f\"single-{key}\",\n",
    "\n",
    "    \"epochs\": n_epochs,\n",
    "\n",
    "    \"patience\": patience,\n",
    "\n",
    "    \"architecture\":\"ConvolutionalDeepSets\",\n",
    "\n",
    "\n",
    "    \"hidden1\" : hidden1,\n",
    "\n",
    "    \"hidden2\" : hidden2,\n",
    "\n",
    "    \"hidden3\" : hidden3,\n",
    "\n",
    "    \"classify1\" : classify1,\n",
    "\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in t:\n",
    "    avg_loss = train(\n",
    "        model=models[key], \n",
    "        optimizer=optimizers[key], \n",
    "        loader=train_loader, \n",
    "        total=len(train_data), \n",
    "        batch_size=batch_size, \n",
    "        leave=bool(epoch == n_epochs - 1),\n",
    "        datatype=key \n",
    "    )\n",
    "    \n",
    "    \n",
    "    valid_loss, valid_acc , csv = test(\n",
    "        model=models[key],\n",
    "        loader=test_loader, \n",
    "        total=len(test_data), \n",
    "        batch_size=batch_size, \n",
    "        leave=bool(epoch == n_epochs - 1),\n",
    "        datatype=key\n",
    "    )\n",
    "    \n",
    "    wandb.log({\"train_loss\": avg_loss, \"valid_loss\": valid_loss, \"valid_acc\": valid_acc})\n",
    "\n",
    "    print(\"Epoch: {:02d}, Training Loss:   {:.4f}\".format(epoch, avg_loss))\n",
    "    print(\"           Validation Loss: {:.4f}\".format(valid_loss))\n",
    "    print(\"           Validation Accuracy: {:.4f}\".format(valid_acc))\n",
    "\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        modpath = osp.join(f\"../models/DeepSets_{key}_{feature}.pth\")\n",
    "        print(\"New best model saved to:\", modpath)\n",
    "        torch.save(models[key].state_dict(), modpath)\n",
    "        # save csv\n",
    "        csv[\"folder\"] = csv[\"folder\"].astype(int)\n",
    "        csv[\"prediction\"] = [\"grant\" if x == 1 else \"deny\" for x in csv[\"prediction\"]]\n",
    "        csv[\"truth\"] = [\"grant\" if x == 1 else \"deny\" for x in csv[\"truth\"]]\n",
    "        \n",
    "        csv.to_csv(f\"../predictions/DeepSets_{key}_{feature}_predictions.csv\", index=False)\n",
    "\n",
    "        stale_epochs = 0\n",
    "    else:\n",
    "        print(\"Stale epoch\")\n",
    "        stale_epochs += 1\n",
    "    if stale_epochs >= patience:\n",
    "        print(\"Early stopping after %i stale epochs\" % patience)\n",
    "        break\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del models[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
