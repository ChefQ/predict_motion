{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note \n",
    "You will have to run this notebook twice. One for Support and the Other for Opposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset , load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding , AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification , TrainingArguments , AutoConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import evaluate\n",
    "import time\n",
    "import wandb\n",
    "import random \n",
    "from transformers import TrainingArguments, Trainer , AutoModelForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from datasets import Features , ClassLabel, Value, Sequence\n",
    "from tqdm.auto import tqdm\n",
    "roberta_checkpoint = \"roberta-large\"\n",
    "\n",
    "mistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "bert_checkpoint = \"allenai/longformer-large-4096\"# \"bert-base-uncased\"  \n",
    "\n",
    "llama_checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "MAX_LEN = 512 \n",
    "from accelerate import Accelerator\n",
    "\n",
    "# device =  torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.distributed.is_available())\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Functions\n",
    "\n",
    "In this tab retrive the dataset relevant for the experiment.\n",
    "You only have to run the cell below once.\n",
    "It is important know that the cell below produces two types of dataframes, \"support\" and \"opposition\".\n",
    "\n",
    "\n",
    "This tab also contains most of the functinos used in this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision2label(decision):\n",
    "    if  \"grant\" in decision:\n",
    "        return 1\n",
    "    elif \"deny\" in decision:\n",
    "        return 0\n",
    "    else:\n",
    "        print(f\"error occured with decision: {decision} \",)\n",
    "        exit(\"Invalid decision\")\n",
    "\n",
    "\n",
    "def test_metrics(model, dataloader):\n",
    "    acc = evaluate.load(\"accuracy\")\n",
    "    preci = evaluate.load(\"precision\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "\n",
    "    csv = {'brief':[],'predict':[], 'score':[], 'truth':[]}\n",
    "\n",
    "    model.eval()\n",
    "    for inputs in dataloader:\n",
    "        briefs = inputs['file_name']\n",
    "        inputs = {k: v for k, v in inputs.items() if k != \"file_name\"}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        propabilities = f.softmax(logits, dim=-1)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        acc.add_batch(predictions=predictions, references=inputs[\"labels\"])\n",
    "        preci.add_batch(predictions=predictions, references=inputs[\"labels\"])\n",
    "        recall.add_batch(predictions=predictions, references=inputs[\"labels\"])\n",
    "\n",
    "        csv['brief'].extend(briefs)\n",
    "        labels = lambda x: \"grant\" if x == 1 else \"deny\"\n",
    "        predict = list(map(labels, predictions))\n",
    "        csv['predict'].extend(predict) \n",
    "        csv['score'].extend(propabilities[:,1].cpu().numpy())\n",
    "        csv['truth'].extend(list(map(labels, inputs[\"labels\"].cpu().numpy())))\n",
    "\n",
    "    return {'accuracy': acc.compute()['accuracy'],\n",
    "            'precision': preci.compute()['precision'], \n",
    "            'recall': recall.compute()['recall'],\n",
    "            'csv': csv}\n",
    "\n",
    "\n",
    "model_type = \"bert\"\n",
    "\n",
    "key = \"support\"\n",
    "\n",
    "# Remember to change this\n",
    "\n",
    "#TESTSET = \"../dataset/testset.csv\"\n",
    "\n",
    "UNPAIRED_PATH = '../dataset/testset.csv'\n",
    "\n",
    "testset = pd.read_csv(UNPAIRED_PATH, index_col=0)\n",
    "\n",
    "testset = testset.loc[testset['data_type'] == 'train']\n",
    "\n",
    "# randomly set 20% of the data to test\n",
    "testset['data_type'] = testset['data_type'].apply(lambda x: \"test\" if random.random() < 0.2 else \"train\")\n",
    "\n",
    "testset['labels'] = testset['completion'].apply(decision2label)\n",
    "\n",
    "train = testset.loc[testset['data_type'] == 'train']\n",
    "test = testset.loc[testset['data_type'] == 'test']\n",
    "\n",
    "support_train = train.loc[train['brief_type'] == \"support\"]\n",
    "support_test = test.loc[test['brief_type'] == \"support\"]\n",
    "\n",
    "oppo_train = train.loc[train['brief_type'] == \"opposition\"]\n",
    "oppo_test = test.loc[test['brief_type'] == \"opposition\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "features = Features({ 'prompt' : Value(dtype='string'),\n",
    "                    'completion': ClassLabel(num_classes=3, names=['deny', 'grant', 'TBD'],  id=None),\n",
    "                    'brief_type' : ClassLabel(num_classes=2, names=[\"support\", \"opposition\"], id=None),\n",
    "                        'data_type' : ClassLabel(num_classes=2, names=[\"train\", \"test\"], id=None),\n",
    "                        'file_path' : Value(dtype='int64') ,\n",
    "                        'file_name' : Value(dtype='string'),   \n",
    "                        'labels' : Value(dtype='int64')\n",
    "                    })\n",
    "\n",
    "# can change the argument\n",
    "if key == \"support\":    \n",
    "    dataset_train = Dataset.from_pandas(support_train, preserve_index=False , features= features )\n",
    "    dataset_test = Dataset.from_pandas(support_test, preserve_index=False,  features= features)\n",
    "else: #  key == opposition\n",
    "\n",
    "    dataset_train = Dataset.from_pandas(oppo_train, preserve_index=False, features= features)\n",
    "    dataset_test = Dataset.from_pandas(oppo_test, preserve_index=False, features= features)\n",
    "\n",
    "\n",
    "\n",
    "dataset = DatasetDict()\n",
    "\n",
    "\n",
    "dataset['train'] = dataset_train\n",
    "dataset['test'] = dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_function():\n",
    "\n",
    "\n",
    "    # try putting support and train together\n",
    "\n",
    "    lr = 1e-5\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    if model_type == \"mistral\":\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint, add_prefix_space=True,)\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "        config = AutoConfig.from_pretrained(mistral_checkpoint)\n",
    "        max_input_size =  1024\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples['prompt'], truncation= True, padding=\"max_length\" , max_length=max_input_size)\n",
    "\n",
    "        #mistral_data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
    "\n",
    "        def tokenize_function(briefs):\n",
    "            return tokenizer(briefs[\"prompt\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"completion\",\"prompt\",\"brief_type\",\"data_type\", \"file_path\",  ])\n",
    "\n",
    "\n",
    "    # tokenized_datasets = tokenized_datasets.remove_columns() # ])\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=1, )\n",
    "    eval_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=1, ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    num_epochs = 50\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "    name = \"Support\" if key == \"support\" else \"Opposition\"\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"LLM_TUTORIAL\",  \n",
    "        name= f\"{name}-{bert_checkpoint}\",#f\"Opposition-mistral-7B-v0.1-1-Tokensize:{max_input_size}\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"lr\": lr,\n",
    "\n",
    "        \"dataset\": \"single-supports\",\n",
    "        \"epochs\": num_epochs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    lr = 1e-5\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(bert_checkpoint, num_labels=2, )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    num_params = model.num_parameters()\n",
    "    print(f\"The model has {num_params} parameters.\")\n",
    "    print(f\"The model has a context window of {model.config.max_position_embeddings} tokens.\")\n",
    "\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    # i wonder if the outputs.loss is the same as loss_fn(outputs, labels)\n",
    "    # Try to log the values \n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    model.train()\n",
    "    print(\"Training model\")\n",
    "    for epoch in range(num_epochs):\n",
    "        acc = evaluate.load(\"accuracy\")\n",
    "        average_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {k: v for k, v in batch.items() if k != \"file_name\"}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            average_loss += loss.item()\n",
    "            \n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            # get the predictions\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            predictions = accelerator.gather(predictions)\n",
    "            labels = accelerator.gather(inputs[\"labels\"])\n",
    "            acc.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "        accuracy_per_epoch = acc.compute()\n",
    "        print(f\"Epoch {epoch} completed\")\n",
    "        print(f\"Accuracy: {accuracy_per_epoch}\")\n",
    "        avg_loss = average_loss / len(train_dataloader)\n",
    "        print(f\"loss : {avg_loss}\")\n",
    "\n",
    "        print(\"Evaluating model on test set\")\n",
    "        metrics = test_metrics(model, eval_dataloader)\n",
    "        csv = metrics[\"csv\"]\n",
    "        csv = pd.DataFrame(csv)\n",
    "        print(metrics)\n",
    "        \n",
    "        wandb.log({\"loss_per_epoch\": avg_loss , \n",
    "                \"accuracy_per_epoch\": accuracy_per_epoch,\n",
    "                \"test_accuracy\" :metrics[\"accuracy\"],\n",
    "                    \"test_recall\": metrics[\"recall\"],\n",
    "                    \"test_precision\": metrics[\"precision\"],\n",
    "                })\n",
    "        \n",
    "        if metrics[\"accuracy\"] > best_valid_acc:\n",
    "            best_valid_acc = metrics[\"accuracy\"]\n",
    "            print(\"Saving model\")\n",
    "            model.save_pretrained(f\"../models/LLM-{model_type}-{key}-test\")\n",
    "            csv.to_csv(f\"../predictions/LLM-{model_type}-{key}-test.csv\", index=False)\n",
    "        \n",
    "    wandb.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6728b4c3c26a4c8ab144d6ccfe8f1939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90a50d7c92e4ca89296b168a89c6dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moqcardoso\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratchB/oqcardoso/predict_motion/per_motion_prediction/wandb/run-20240409_152626-0zy1mv5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/0zy1mv5v' target=\"_blank\">Support-allenai/longformer-large-4096</a></strong> to <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL' target=\"_blank\">https://wandb.ai/oqcardoso/LLM_TUTORIAL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/0zy1mv5v' target=\"_blank\">https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/0zy1mv5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moqcardoso\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratchB/oqcardoso/predict_motion/per_motion_prediction/wandb/run-20240409_152627-e60md4so</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/e60md4so' target=\"_blank\">Support-allenai/longformer-large-4096</a></strong> to <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL' target=\"_blank\">https://wandb.ai/oqcardoso/LLM_TUTORIAL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/e60md4so' target=\"_blank\">https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/e60md4so</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-large-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 434603010 parameters.\n",
      "The model has a context window of 4098 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-large-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 434603010 parameters.\n",
      "The model has a context window of 4098 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5399235fd7490ba2669ef00b1f8014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0014633c874eb39509993cce354791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "An issue was found when launching the training: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/accelerate/utils/launch.py\", line 570, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_24318/573297226.py\", line 90, in training_function\n    outputs = model(**inputs)\n              ^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1925, in forward\n    outputs = self.longformer(\n              ^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1738, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1318, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1246, in forward\n    self_attn_outputs = self.attention(\n                        ^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1182, in forward\n    self_outputs = self.self(\n                   ^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 637, in forward\n    attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 91.38 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 650.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/accelerate/launchers.py:200\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:158\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    157\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/accelerate/utils/launch.py\", line 570, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_24318/573297226.py\", line 90, in training_function\n    outputs = model(**inputs)\n              ^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1925, in forward\n    outputs = self.longformer(\n              ^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1738, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1318, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1246, in forward\n    self_attn_outputs = self.attention(\n                        ^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1182, in forward\n    self_outputs = self.self(\n                   ^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 637, in forward\n    attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 91.38 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 650.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/accelerate/launchers.py:210\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    204\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA has been initialized before the `notebook_launcher` could create a forked subprocess. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis likely stems from an outside import causing issues once the `notebook_launcher()` is called. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease review your imports and test them when running the `notebook_launcher()` to identify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich one is problematic and causing CUDA to be initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    209\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn issue was found when launching the training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_mps_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: An issue was found when launching the training: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/accelerate/utils/launch.py\", line 570, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_24318/573297226.py\", line 90, in training_function\n    outputs = model(**inputs)\n              ^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1925, in forward\n    outputs = self.longformer(\n              ^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1738, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1318, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1246, in forward\n    self_attn_outputs = self.attention(\n                        ^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1182, in forward\n    self_outputs = self.self(\n                   ^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/longformer/modeling_longformer.py\", line 637, in forward\n    attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 91.38 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 650.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "notebook_launcher(training_function, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
